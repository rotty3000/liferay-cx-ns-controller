# The Liferay CX Controller Manager

This project maintains a set of controllers that implement the [Liferay Client Extension Protocol](https://liferay.atlassian.net/wiki/x/qwQGeg).

There are 4 processes involved in this protocol:

- [x] management of DXP Metadata ConfigMaps which mirror the Virtual Instances inside DXP
- [x] management of Extension Namespaces related to DXP Metadata ConfigMaps in order to house Client Extension workloads
- [ ] management of Provisioning Metadata ConfigMaps such that deployment of CX is discovered by DXP
- [ ] management of Init Metadata ConfigMaps when DXP responds to a Provisioned Metadata ConfigMap with CX specific data

## DXP Metadata ConfigMaps Controller

The first controller is called the DXP Metadata ConfigMaps Controller.

### Overview of the controller

When the Liferay DXP Kubernetes Agent is enabled a ConfigMap (DXP Metadata ConfigMap) that describes the Virtual Instance is emitted and maintained for every Virtual Instance existing in DXP.

When the Virtual Instance is updated the ConfigMap is updated and when a Virtual Instance is deleted the ConfigMap is removed.

The controller ensures the existence of at least one "default" Extension Namespace per DXP Metadata ConfigMap in order to provide a deployment area for Client Extensions that target the Virtual Instance. The "default" Extension Namespace _name_ will be autogenerated to reflect the deployment namespace of DXP, the Virtual Instance ID and an application alias (which by default is `default`). This alias allows multiple Extension Namespaces to exist in parallel associated with each Virtual Instance. This architecture allows for Marketplace Apps to be isolated into their own Namespace while still being associated to the Virtual Instance.

## Extension Namespace Controller

The second controller is called the Extension Namespace Controller.

### Overview of the controller

The DXP Metadata ConfigMaps Controller creates a "default" Extension Namespace for every DXP Metadata ConfigMap. However we need to account for additional Extension Namespaces per Virtual Instance to support cases like Marketplace Apps.

The Extension Namespace Controller reacts to the creation of Extension Namespaces and binds them to the Virtual Instance by adding appropriate metadata to the Namespace and by copying the DXP Metadata ConfigMap to it. Beyond this point the Extension Namespace will be managed and kept up-to-date by the DXP Metadata ConfigMap Controller.

## Provisioning Metadata ConfigMap Controller

The third controller is called the Provisioning Metadata ConfigMap Controller.

### Overview of the controller

When a CX helm chart is deployed into an Extension Namespace a Provisioning Metadata ConfigMap is created. In order for DXP to discover the CX the Provisioning Metadata ConfigMap needs to be copied into the DXP Namespace.

The controller will copy the Provisioning Metadata ConfigMap into the DXP Namespace and perform any updates and delete the copied Provisioning Metadata ConfigMap when the CX is deleted.

## Init Metadata ConfigMap Controller

The fourth controller is called the Init Metadata ConfigMap Controller.

### Overview of the controller

Based on the contents of the Provisioning Metadata ConfigMap, DXP may emit an Init Metadata ConfigMap into it's own namespace. This ConfigMap contains CX specific configuration and must be copied to the Extension Namespace of the CX in order for Kubernetes to schedule the CX workload.

The controller will copy the Init Metadata ConfigMap into the Extension Namespace and perform any updates and delete the copied Init Metadata ConfigMap when DXP deletes the ConfigMap in the DXP namespace.

## Testing

### Deploy the locally build controller manager into a K3d cluster

Start by installing a basic Liferay DXP setup:

```bash
# if from source
helm upgrade -i -n liferay-system --create-namespace liferay . \
	--set image.tag=7.4.13.nightly

# if from registry
helm upgrade -i -n liferay-system --create-namespace liferay \
	oci://us-central1-docker.pkg.dev/liferay-artifact-registry/liferay-helm-chart/liferay-default \
	--set image.tag=7.4.13.nightly
```

Watch DXP:

```bash
watch -n .5 kubectl -n liferay-system get statefulset,pod,pvc,svc,ingress,cm,secret -o wide
```

Install the controller:

```bash
# build the image
make docker-build

# tag the image to push it into the k3d registry
docker tag liferay-cx-ns-controller:latest registry:5000/liferay-cx-ns-controller:latest

# push the image into the k3d registry
docker push registry:5000/liferay-cx-ns-controller:latest

# deploy the controller into the k3d cluster (the namespace is determined by `config/default/kustomization.yaml` which is `liferay-cx-ns-controller-system` by default)
make deploy IMG=registry:5000/liferay-cx-ns-controller:latest
```

Watch any namespaces managed by the controller:

```bash
watch -n .5 kubectl get ns --selector "app.kubernetes.io/managed-by=liferay-cx-ns-controller"
```

Watch any DXP metadata configmaps sync into those namespaces by the controller:

```bash
watch -n .5 kubectl get -A cm --selector "cx.liferay.com/synced-from-configmap"
```

Create the ingress to reach Liferay DXP:

```bash
kubectl -n liferay-system create ingress liferay --rule="*.dxp.docker.localhost/*=liferay-default:http"
```

Login to Liferay DXP (`test@liferay.com/test`) and create a Virtual Instance:

```bash
open http://main.dxp.docker.localhost
```

Watch as the controller creates the managed default namespace for the VI and syncs the DXP metadata ConfigMap into it.
